adam_beta1: 0.9
adam_beta2: 0.999
adjust_step: 0
autoresume: false
batch_size: 24
clip_grad_norm: 1.0
comment: null
cycle_length: 300
dataset_path: ./preprocessed_data/c4_t5-base_512/
distributed_type: ddp
dtype: float32
eval_every: 200
force_keep_original: false
gradient_accumulation: 12
keep_checkpoints: null
load_optimizer_state_on_resume: true
lora_alpha: 32
lora_r: null
lr: 0.001
max_length: 512
max_train_tokens: null
megatron_dataset_config: null
min_lr_ratio: 0.1
model_config: configs/llama_9m.json
model_name_or_path: null
model_revision: null
num_training_steps: 600
optimizer: Adam
optimizer_magnitude_pruning: 0.0
optimizer_random_pruning: 0.0
profile: false
relora: null
reset_optimizer_on_relora: true
restart_warmup_steps: 20
resume_from: null
run_name: ''
save_dir: ./output20241013111633/
save_every: 200
scheduler: cosine_restarts
seed: 0
skip_batches: !!set {}
tags:
- relora_9M
total_batch_size: 1152
train_scaling: false
training_config: null
use_peft: false
wandb_watch: false
warmed_up_model: ./checkpoints/model_611/
warmup_steps: 30
weight_decay: 0.0
workers: 8
